# Transformers for Russian school text complexity prediction

We experiment with fine-tuning pre-trained BERT models to classify the complexity of Russian school text using different fragment sizes for training. Our motivation is characterizing the effect of different training fragment lengths on the performance and the training time. The ultimate goal is to optimize the training time without significantly hurting the performance of the model.

Dataset link: ![here](https://drive.google.com/drive/folders/1ZeXGnPNASJCXKPpenU05IFxoPrDy4C_Q)


